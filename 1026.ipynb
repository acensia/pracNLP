{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 word imbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n",
      "총 샘플 수 : 11004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수 :',len(documents))\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "news_df.isnull().values.any()\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()\n",
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :',len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choijunhyuk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11004\n",
      "43\n",
      "총 샘플 수 : 10961\n",
      "[[40, 53, 927, 143, 15889, 1684, 546, 279, 871, 12028, 17773, 24007, 29726, 279, 871, 63435, 871, 1128, 1103, 1998, 851, 29727, 913, 731, 20477, 279, 871, 170, 143, 1811, 149, 279, 20478, 17773, 6645, 5710, 76, 63436, 7, 36, 165, 614, 653, 29728, 6911, 24008, 2082, 829, 17774, 1119, 8790, 355, 1072, 15890, 671, 57, 163, 4231, 7206, 1933, 440, 56, 282, 4730, 9275, 2690, 39306], [1283, 429, 3, 52, 6164, 159, 112, 474, 89, 17775, 18, 63, 4731, 2865, 63437, 1042, 402, 39307, 8791, 902, 44, 8328, 316, 13041, 902, 3452, 5923, 533, 18, 87, 4732, 9872, 160, 1403, 120, 151, 5194, 63438, 63439, 17776, 63440, 13041, 903, 63441, 63442, 11172, 17777]]\n",
      "단어 집합의 크기 : 181839\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "print(f\"{len(tokenized_doc)}\")\n",
    "print(len(drop_train))\n",
    "tokenized_doc = [sentence for index, sentence in enumerate(tokenized_doc) if index not in drop_train]\n",
    "print('총 샘플 수 :',len(tokenized_doc))\n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
    "print(encoded[:2])\n",
    "vocab_size = len(word2idx) + 1\n",
    "print('단어 집합의 크기 :', vocab_size)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "# for i in range(5):\n",
    "#     print(\"({:s} ({:d}), {:s} ({:d})\".format(\n",
    "#         idx2word[pairs[i][0]]\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/choijunhyuk/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x153e3cdc0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding=\"UTF8\")\n",
    "target_text = etree.parse(targetXML)\n",
    "parse_text = \"\\n\".join(target_text.xpath(\"//content/text()\"))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "sent_text = sent_tokenize(content_text)\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "res = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample numbers : 273424\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sample numbers : {}\".format(len(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# model = Word2Vec(sentences=res, vector_size=100, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(res, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.8675845265388489),\n",
       " ('electrolyte', 0.8672700524330139),\n",
       " ('electroshock', 0.854694128036499),\n",
       " ('electro', 0.8530812859535217),\n",
       " ('electroencephalogram', 0.8423311114311218),\n",
       " ('electrogram', 0.8274851441383362),\n",
       " ('electrochemical', 0.8241797685623169),\n",
       " ('electric', 0.8189011812210083),\n",
       " ('electrode', 0.8187267780303955),\n",
       " ('electromagnet', 0.8159869909286499)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import hgtk\n",
    "from konlpy.tag import Mecab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 리뷰 개수 : 200000\n"
     ]
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n",
    "\n",
    "total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])\n",
    "print('전체 리뷰 개수 :',len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgtk.checker.is_hangul('ㄱ')\n",
    "hgtk.checker.is_hangul('28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'남'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgtk.letter.compose('ㄴ', 'ㅏ', 'ㅁ')\n",
    "# hgtk.letter.compose('ㄴ', 'ㅏ', 'ㅁ', 'ㅁ') // parameter numbers error\n",
    "# hgtk.letter.compose('ㄴ', 'ㅏ', 'ㅏ') //invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅇㅕ-ㄷㅗㅇㅅㅐㅇ'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_jamo(token):\n",
    "    def to_special_token(jamo):\n",
    "        if not jamo: return '-'\n",
    "        else:\n",
    "            return jamo\n",
    "    decomposed_token = ''\n",
    "    for char in token:\n",
    "        try:\n",
    "            cho, jung, jong = hgtk.letter.decompose(char)\n",
    "            cho = to_special_token(cho)\n",
    "            jung = to_special_token(jung)\n",
    "            jong = to_special_token(jong)\n",
    "            decomposed_token = decomposed_token + cho + jung + jong\n",
    "        except Exception as exception:\n",
    "            if type(exception).__name__ == 'NotHangulException':\n",
    "                decomposed_token += char\n",
    "    return decomposed_token\n",
    "\n",
    "word_to_jamo('남동생')\n",
    "word_to_jamo('여동생')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement 'mecab_python-0.996_ko_0.9.2_msvc-cp37-cp37m-win_amd64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: mecab_python-0.996_ko_0.9.2_msvc-cp37-cp37m-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pracNLP/lib/python3.8/site-packages/konlpy/tag/_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagger \u001b[39m=\u001b[39m Tagger(\u001b[39m'\u001b[39m\u001b[39m-d \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/data/tagset/mecab.json\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m utils\u001b[39m.\u001b[39minstallpath)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/choijunhyuk/Desktop/forVScode/NLP/1026.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/choijunhyuk/Desktop/forVScode/NLP/1026.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install mecab_python-0.996_ko_0.9.2_msvc-cp37-cp37m-win_amd64.whl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/choijunhyuk/Desktop/forVScode/NLP/1026.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mecab \u001b[39m=\u001b[39m Mecab(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mmecab\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mmecab-ko-dic\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/choijunhyuk/Desktop/forVScode/NLP/1026.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(mecab\u001b[39m.\u001b[39mmorphs(\u001b[39m'\u001b[39m\u001b[39m선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/choijunhyuk/Desktop/forVScode/NLP/1026.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_by_jamo\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/pracNLP/lib/python3.8/site-packages/konlpy/tag/_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe MeCab dictionary does not exist at \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Is the dictionary correctly installed?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMecab(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m/some/dic/path\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[1;32m     81\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "!pip install mecab_python-0.996_ko_0.9.2_msvc-cp37-cp37m-win_amd64.whl\n",
    "\n",
    "mecab = Mecab(r'C:\\mecab\\mecab-ko-dic')\n",
    "print(mecab.morphs('선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.'))\n",
    "def tokenize_by_jamo(s):\n",
    "    return [word_to_jamo(token) for token in mecab.morphs(s)]\n",
    "print(tokenize_by_jamo('선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.'))\n",
    "tokenized_data = []\n",
    "for sample in tqdm(total_data['reviews'].to_list()):\n",
    "    tokenzied_sample = tokenize_by_jamo(sample)\n",
    "    tokenized_data.append(tokenzied_sample)\n",
    "len(tokenized_data)\n",
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'남동생'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jamo_to_word(jamo_sequence):\n",
    "  tokenized_jamo = []\n",
    "  index = 0\n",
    "  while index < len(jamo_sequence):\n",
    "    if not hgtk.checker.is_hangul(jamo_sequence[index]):\n",
    "      tokenized_jamo.append(jamo_sequence[index])\n",
    "      index = index + 1\n",
    "    else:\n",
    "      tokenized_jamo.append(jamo_sequence[index:index + 3])\n",
    "      index = index + 3\n",
    "  word = ''\n",
    "  try:\n",
    "    for jamo in tokenized_jamo:\n",
    "      if len(jamo) == 3:\n",
    "        if jamo[2] == \"-\":\n",
    "          word = word + hgtk.letter.compose(jamo[0], jamo[1])\n",
    "        else:\n",
    "          word = word + hgtk.letter.compose(jamo[0], jamo[1], jamo[2])\n",
    "      else:\n",
    "        word = word + jamo\n",
    "  except Exception as exception:\n",
    "    if type(exception).__name__ == 'NotHangulException':\n",
    "      return jamo_sequence\n",
    "  return word\n",
    "\n",
    "jamo_to_word(word_to_jamo('남동생'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toknized_data = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pracNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
